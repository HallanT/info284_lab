{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thale Knudsen Kirkhorn and Anne Jacobsen Rike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=read_csv('Tweets.csv')\n",
    "tweets = data['text']\n",
    "labels = data['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweets): \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Updating list of stopwords to include some words without contractions \n",
    "    stop_words.update({'youre', 'youll', 'youd', 'hadnt', 'wouldnt'})\n",
    "    \n",
    "    def fix_tweet(tweet): \n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "        tweet = re.sub(\"\\d+\", '', tweet)\n",
    "    \n",
    "        tweet = tweet.split()\n",
    "        tweet = [word for word in tweet if word not in stop_words] \n",
    "        tweet = [word for word in tweet if not word.startswith('http')]\n",
    "        return tweet\n",
    "    tweets = tweets.apply(fix_tweet)\n",
    "    return tweets\n",
    "\n",
    "def split(tweets, labels):\n",
    "    train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweets, labels, test_size=0.25, random_state=42)\n",
    "    return train_tweets, test_tweets, train_labels, test_labels\n",
    "\n",
    "def build_vocab(train_tweets): \n",
    "    vocab = set()\n",
    "    for tweet in train_tweets: \n",
    "        for word in tweet: \n",
    "            vocab.add(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = clean(tweets)\n",
    "train_tweets, test_tweets, train_labels, test_labels = split(tweets, labels)\n",
    "vocab = build_vocab(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3. One might make use of some of the metadata fields present in the dataset, however we do not see this \n",
    "# as necessary in this assignment. If we were to use the metadata, we could have used the \"negativereason\", and \n",
    "# the \"name\" columns, as this could have improved the accurancy by seeing the context of a tweet. For example, \n",
    "# if a person usually writes a negative tweet, one can assume that their tweets usually tend to be negative. \n",
    "# As for the \"negativereason\", one could find out if for example someone writes a positive sarcastic tweet,  \n",
    "# but tagging it as a \"bad flight\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes: \n",
    "    def __init__(self): \n",
    "        self.n_negative = None\n",
    "        self.n_neutral = None \n",
    "        self.n_positive = None\n",
    "        self.n_total = None \n",
    "        self.negative_prior = None\n",
    "        self.neutral_prior = None\n",
    "        self.positive_prior = None \n",
    "        self.most_negative = None\n",
    "        self.sentiment_classes = None\n",
    "        self.counts = None\n",
    "        self.total_counts = None\n",
    "        self.likelihood = None\n",
    "        self.probs = None\n",
    "        self.prediction = None\n",
    "    \n",
    "    def train(self, train_tweets, train_labels, vocab):\n",
    "        # Task 4, prior: \n",
    "        self.n_negative = train_labels.value_counts()[0]\n",
    "        self.n_neutral = train_labels.value_counts()[1]\n",
    "        self.n_positive = train_labels.value_counts()[2]\n",
    "        self.n_total = self.n_negative + self.n_neutral + self.n_positive \n",
    "        self.negative_prior = np.log(self.n_negative / self.n_total)\n",
    "        self.neutral_prior = np.log(self.n_neutral / self.n_total)\n",
    "        self.positive_prior = np.log(self.n_positive / self.n_total)\n",
    "        \n",
    "        # Counts the number of times a word occurs in a text that is negative/neutral/positive \n",
    "        self.counts = {'negative':defaultdict(int), 'neutral':defaultdict(int), 'positive':defaultdict(int)}\n",
    "        for tweet, label in zip(train_tweets, train_labels): \n",
    "            for word in tweet:\n",
    "                self.counts[label][word] += 1\n",
    "        \n",
    "        # Counts the total amount of words in both positive and negative \n",
    "        self.sentiment_classes = ['negative', 'neutral', 'positive']\n",
    "        self.total_counts = {'negative':0, 'neutral':0, 'positive':0} \n",
    "        for sentiment in self.sentiment_classes:\n",
    "            for word in vocab: \n",
    "                self.total_counts[sentiment] += self.counts[sentiment][word] \n",
    "        \n",
    "        # Calculates the likelihood (Task 8, Laplace smoothing)\n",
    "        self.likelihood = {'negative':{}, 'neutral':{}, 'positive':{}} \n",
    "        for sentiment in self.sentiment_classes: \n",
    "            for word in vocab: \n",
    "                self.likelihood[sentiment][word] = np.log((self.counts[sentiment][word]+1) / (self.total_counts[sentiment]+len(vocab)))        \n",
    "        \n",
    "    def predict(self, tweet): \n",
    "        self.probs = {'negative':self.negative_prior, 'neutral':self.neutral_prior, 'positive':self.positive_prior}\n",
    "        for sentiment in self.sentiment_classes:\n",
    "            for word in tweet: \n",
    "                if word in vocab: \n",
    "                    self.probs[sentiment] += self.likelihood[sentiment][word]\n",
    "        \n",
    "        values = list(self.probs.values())\n",
    "        best_index = values.index(max(values))\n",
    "        return self.sentiment_classes[best_index]\n",
    "    \n",
    "    def explanation_generator(self, tweet):\n",
    "        print('This text was predicted to be', self.predict(tweet))\n",
    "        for word in tweet:\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            predicted_class = None\n",
    "            highest_value = -float('inf')\n",
    "            for sentiment in self.sentiment_classes: \n",
    "                if self.likelihood[sentiment][word] > highest_value:\n",
    "                    predicted_class = sentiment\n",
    "                    highest_value = self.likelihood[sentiment][word]\n",
    "            print(word, predicted_class)                \n",
    "    \n",
    "    def evaluate(self, test_tweets, test_labels): \n",
    "        # Test accuracy\n",
    "        n_correct = 0 \n",
    "        n_total_test = sum(test_labels.value_counts())\n",
    "        i = 0\n",
    "        for tweet, label in zip(test_tweets, test_labels): \n",
    "            prediction = self.predict(tweet)\n",
    "    \n",
    "            if prediction == label: \n",
    "                n_correct += 1 \n",
    "                \n",
    "        print(n_correct, n_total_test)\n",
    "        return n_correct/n_total_test\n",
    "    \n",
    "    \n",
    "    # Task 11, finding correctly and incorrectly predicted tweets  \n",
    "    def find_wrongly_classified(self, test_tweets, test_labels): \n",
    "        # Test accuracy\n",
    "        n_correct = 0 \n",
    "        n_total_test = sum(test_labels.value_counts())\n",
    "        for (ind, tweet), label in zip(test_tweets.items(), test_labels): \n",
    "            prediction = self.predict(tweet)\n",
    "            if prediction == label: \n",
    "                n_correct += 1 \n",
    "            else: \n",
    "                print(ind)\n",
    "    \n",
    "    def find_correctly_classified(self, test_tweets, test_labels): \n",
    "        # Test accuracy\n",
    "        n_correct = 0 \n",
    "        n_total_test = sum(test_labels.value_counts())\n",
    "        for (ind, tweet), label in zip(test_tweets.items(), test_labels): \n",
    "            prediction = self.predict(tweet)\n",
    "            if prediction == label: \n",
    "                print(ind) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data[tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2857 3660\n",
      "0.7806010928961749\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.train(train_tweets, train_labels, vocab)\n",
    "accuracy = nb.evaluate(test_tweets, test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your text: it was a terrible flight\n",
      "Your text is: negative\n"
     ]
    }
   ],
   "source": [
    "# Task 9, command line utility\n",
    "\n",
    "def fix_tweet(tweet): \n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "        tweet = re.sub(\"\\d+\", '', tweet)\n",
    "        \n",
    "        tweet = tweet.split()\n",
    "        tweet = [word for word in tweet if not word.startswith('http')]\n",
    "        return tweet\n",
    "    \n",
    "text = input('Your text: ')\n",
    "text = fix_tweet(text)\n",
    "\n",
    "prediction = nb.predict(text)\n",
    "print(f'Your text is: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text was predicted to be positive\n",
      "southwestair neutral\n",
      "dont negative\n",
      "conference positive\n",
      "number neutral\n",
      "spoke positive\n",
      "texas positive\n",
      "david neutral\n",
      "finally positive\n",
      "fixed positive\n",
      "mine positive\n",
      "hour negative\n"
     ]
    }
   ],
   "source": [
    "# Task 10, explanation generator\n",
    "# For each word, prints the class with maximum likelihood \n",
    "\n",
    "nb.explanation_generator(test_tweets[6209])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrongly classified: (['united', 'luckily', 'disinfectant', 'wipes', 'job', 'welcome', 'badservice'], 'negative')\n",
      "Tweet 2372 predicted as: positive\n",
      "\n",
      "Wrongly classified: (['southwestair', 'dont', 'conference', 'number', 'spoke', 'courtney', 'texas', 'david', 'finally', 'fixed', 'mine', 'hour'], 'negative')\n",
      "Tweet 6209 predicted as: positive\n",
      "\n",
      "Correctly classified: (['usairways', 'going', 'make', 'wait', 'phone', 'long', 'time', 'least', 'turn', 'ads', 'eating', 'away', 'soul'], 'negative')\n",
      "Tweet 9924 predicted as: negative\n",
      "\n",
      "Correctly classified: (['southwestair', 'werent', 'sincere', 'cancelled', 'flighted', 'flight', 'made', 'drive', 'hours', 'get', 'home'], 'negative')\n",
      "Tweet 4664 predicted as: negative\n"
     ]
    }
   ],
   "source": [
    "# Task 11, correctly and incorrectly predicted tweets  \n",
    "\n",
    "#nb.find_wrongly_classified(test_tweets, test_labels)\n",
    "#nb.find_correctly_classified(test_tweets, test_labels)\n",
    "\n",
    "print(f'Wrongly classified: {test_tweets[2372], test_labels[2372]}')\n",
    "print('Tweet 2372 predicted as: ' + nb.predict(test_tweets[2372]) + \"\\n\")\n",
    "# Tweet 2372 is probably predicted as positive because of positive words, such as \"luckily\", \"disinfectant\",\n",
    "# and \"welcome\".\n",
    "\n",
    "print(f'Wrongly classified: {test_tweets[6209], test_labels[6209]}')\n",
    "print('Tweet 6209 predicted as: ' + nb.predict(test_tweets[6209]) + \"\\n\")\n",
    "# Tweet 6209 is probably predicted to be positive due to positive words, such as \"finally\" and \"fixed\". \n",
    "\n",
    "print(f'Correctly classified: {test_tweets[9924], test_labels[9924]}')\n",
    "print('Tweet 9924 predicted as: ' + nb.predict(test_tweets[9924]) + \"\\n\")\n",
    "# Tweet 9924 is probably predicted to be negative due to negative words such as \"wait\", \"long\", and \"away\".\n",
    "\n",
    "print(f'Correctly classified: {test_tweets[4664], test_labels[4664]}')\n",
    "print('Tweet 4664 predicted as: ' + nb.predict(test_tweets[4664]))\n",
    "# Tweet 4664 is probably predicted to be negative because of negative words such as \"werent\", \n",
    "# \"cancelled\" and \"hours\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
